{"version":3,"sources":["../src/no-doubled-joshi.js"],"names":[],"mappings":";AACA;;;;;;kBA+De,UAAU,OAAV,EAAiC;AAAA,QAAd,OAAc,yDAAJ,EAAI;;AAC5C,QAAM,SAAS,mCAAe,OAAf,CAAf;;AAEA,QAAM,cAAc,QAAQ,YAAR,IAAwB,eAAe,YAA3D;AACA,QAAM,WAAW,QAAQ,MAAR,IAAkB,eAAe,MAAlD;AACA,QAAM,QAAQ,QAAQ,KAAR,IAAiB,eAAe,KAA9C;AAL4C,QAMrC,MANqC,GAMG,OANH,CAMrC,MANqC;AAAA,QAM7B,MAN6B,GAMG,OANH,CAM7B,MAN6B;AAAA,QAMrB,SANqB,GAMG,OANH,CAMrB,SANqB;AAAA,QAMV,SANU,GAMG,OANH,CAMV,SANU;;AAO5C,+BACK,OAAO,SADZ,YACuB,IADvB,EAC4B;AACpB,YAAI,OAAO,WAAP,CAAmB,IAAnB,EAAyB,CAAC,OAAO,IAAR,EAAc,OAAO,KAArB,EAA4B,OAAO,UAAnC,EAA+C,OAAO,QAAtD,CAAzB,CAAJ,EAA+F;AAC3F;AACH;AACD,YAAM,SAAS,mCAAiB,IAAjB,CAAf;AACA,YAAM,OAAO,OAAO,QAAP,EAAb;AACA,YAAM,iBAAiB,SAAjB,cAAiB,OAAQ;AAC3B,mBAAO,KAAK,IAAL,KAAc,yBAAe,QAApC;AACH,SAFD;AAGA,YAAI,YAAY,6BAAe,IAAf,EAAqB;AACjC,wBAAY;AADqB,SAArB,EAEb,MAFa,CAEN,cAFM,CAAhB;AAGA,eAAO,+BAAe,IAAf,CAAoB,qBAAa;AACpC,gBAAM,gBAAgB,SAAhB,aAAgB,CAAC,QAAD,EAAc;AAChC,oBAAI,SAAS,UAAU,mBAAV,CAA8B,SAAS,GAAvC,CAAb;AACA,oBAAI,kBAAkB,OAAO,MAAP,CAAc,iBAAS;AACzC,wBAAI,QAAJ,EAAc;AACV,+BAAO,2BAAU,KAAV,CAAP;AACH;;;;AAID,2BAAO,2BAAU,KAAV,KAAoB,2BAAU,KAAV,CAA3B;AACH,iBARqB,CAAtB;AASA,oBAAI,0BAA0B,oBAAoB,eAApB,CAA9B;;;;;;;;;AAUA,uBAAO,IAAP,CAAY,uBAAZ,EAAqC,OAArC,CAA6C,eAAO;AAChD,wBAAM,SAAS,wBAAwB,GAAxB,CAAf;AACA,wBAAM,YAAY,yCAAwB,GAAxB,CAAlB;;AAEA,wBAAI,MAAM,OAAN,CAAc,SAAd,KAA4B,CAAhC,EAAmC;AAC/B;AACH;;AAED,wBAAI,CAAC,QAAL,EAAe;AACX,4BAAI,mBAAmB,MAAnB,CAAJ,EAAgC;AAC5B;AACH;AACJ;AACD,wBAAI,OAAO,MAAP,IAAiB,CAArB,EAAwB;AACpB,+B;AACH;;;AAGD,2BAAO,MAAP,CAAc,UAAC,IAAD,EAAO,OAAP,EAAmB;AAC7B,4BAAM,gBAAgB,gBAAgB,OAAhB,CAAwB,IAAxB,CAAtB;AACA,4BAAM,gBAAgB,gBAAgB,OAAhB,CAAwB,OAAxB,CAAtB;;AAEA,4BAAM,kBAAkB,gBAAgB,aAAxC;AACA,4BAAI,mBAAmB,WAAvB,EAAoC;AAChC,gCAAM,mBAAmB,OAAO,mBAAP,CAA2B;AAChD,sCAAM,SAAS,GAAT,CAAa,KAAb,CAAmB,IADuB;AAEhD,wCAAQ,SAAS,GAAT,CAAa,KAAb,CAAmB,MAAnB,IAA6B,QAAQ,aAAR,GAAwB,CAArD;AAFwC,6BAA3B,CAAzB;;AAKA,gCAAM,UAAU;AACZ,sCAAM,iBAAiB,IAAjB,GAAwB,CADlB;;;AAIZ,wCAAQ,iBAAiB;AAJb,6BAAhB;AAMA,mCAAO,IAAP,EAAa,IAAI,SAAJ,yBAAmC,SAAnC,mBAA2D,OAA3D,CAAb;AACH;AACD,+BAAO,OAAP;AACH,qBApBD;AAqBH,iBAvCD;AAwCH,aA7DD;AA8DA,sBAAU,OAAV,CAAkB,aAAlB;AACH,SAhEM,CAAP;AAiEH,KA9EL;AAgFH,C;;AArJD;;AACA;;AACA;;AACA;;;;AACA;;;;;;;;;;;;;;;AAaA,SAAS,mBAAT,CAA6B,MAA7B,EAAqC;;AAEjC,WAAO,OAAO,MAAP,wBAAyB,MAAzB,CAAgC,UAAC,MAAD,EAAS,KAAT,EAAmB;;AAEtD,YAAM,WAAW,kCAAiB,KAAjB,CAAjB;AACA,YAAI,CAAC,OAAO,QAAP,CAAL,EAAuB;AACnB,mBAAO,QAAP,IAAmB,EAAnB;AACH;AACD,eAAO,QAAP,EAAiB,IAAjB,CAAsB,KAAtB;AACA,eAAO,MAAP;AACH,KARM,EAQJ,EARI,CAAP;AASH;AACD,SAAS,kBAAT,CAA4B,MAA5B,EAAoC;AAChC,QAAI,QAAQ,OAAO,CAAP,CAAZ;;AAEA,QAAI,MAAM,YAAN,KAAuB,KAA3B,EAAkC;AAC9B,eAAO,IAAP;AACH;;AAED,QAAI,MAAM,YAAN,KAAuB,KAAvB,IAAgC,MAAM,YAAN,KAAuB,GAA3D,EAAgE;AAC5D,eAAO,IAAP;AACH;;AAED,QAAI,MAAM,YAAN,KAAuB,MAAvB,IAAiC,MAAM,YAAN,KAAuB,GAA5D,EAAiE;AAC7D,eAAO,IAAP;AACH;AACD,WAAO,KAAP;AACH;;;;AAID,IAAM,iBAAiB;AACnB,kBAAc,CADK;AAEnB,YAAQ,KAFW;AAGnB,WAAO;AAHY,CAAvB;;;;;;;;;;AAqGC","file":"no-doubled-joshi.js","sourcesContent":["// LICENSE : MIT\n\"use strict\";\nimport {RuleHelper} from \"textlint-rule-helper\";\nimport {getTokenizer} from \"kuromojin\";\nimport {split as splitSentences, Syntax as SentenceSyntax} from \"sentence-splitter\";\nimport StringSource from \"textlint-util-to-string\";\nimport {\n    is助詞Token, is読点Token,\n    createKeyFromKey, restoreToSurfaceFromKey\n} from \"./token-utils\";\n/**\n * Create token map object\n * {\n *  \"で\": [token, token],\n *  \"の\": [token, token]\n * }\n * @param tokens\n * @returns {*}\n */\nfunction createSurfaceKeyMap(tokens) {\n    // 助詞のみを対象とする\n    return tokens.filter(is助詞Token).reduce((keyMap, token) => {\n        // \"は:助詞.係助詞\" : [token]\n        const tokenKey = createKeyFromKey(token);\n        if (!keyMap[tokenKey]) {\n            keyMap[tokenKey] = [];\n        }\n        keyMap[tokenKey].push(token);\n        return keyMap;\n    }, {});\n}\nfunction matchExceptionRule(tokens) {\n    let token = tokens[0];\n    // \"の\" の重なりは例外\n    if (token.pos_detail_1 === \"連体化\") {\n        return true;\n    }\n    // \"を\" の重なりは例外\n    if (token.pos_detail_1 === \"格助詞\" && token.surface_form === \"を\") {\n        return true;\n    }\n    // 接続助詞 \"て\" の重なりは例外\n    if (token.pos_detail_1 === \"接続助詞\" && token.surface_form === \"て\") {\n        return true;\n    }\n    return false;\n}\n/*\n default options\n */\nconst defaultOptions = {\n    min_interval: 1,\n    strict: false,\n    allow: []\n};\n\n/*\n 1. Paragraph Node -> text\n 2. text -> sentences\n 3. tokenize sentence\n 4. report error if found word that match the rule.\n\n TODO: need abstraction\n */\nexport default function (context, options = {}) {\n    const helper = new RuleHelper(context);\n    // 最低間隔値\n    const minInterval = options.min_interval || defaultOptions.min_interval;\n    const isStrict = options.strict || defaultOptions.strict;\n    const allow = options.allow || defaultOptions.allow;\n    const {Syntax, report, getSource, RuleError} = context;\n    return {\n        [Syntax.Paragraph](node){\n            if (helper.isChildNode(node, [Syntax.Link, Syntax.Image, Syntax.BlockQuote, Syntax.Emphasis])) {\n                return;\n            }\n            const source = new StringSource(node);\n            const text = source.toString();\n            const isSentenceNode = node => {\n                return node.type === SentenceSyntax.Sentence;\n            };\n            let sentences = splitSentences(text, {\n                charRegExp: /[。\\?\\!？！]/\n            }).filter(isSentenceNode);\n            return getTokenizer().then(tokenizer => {\n                const checkSentence = (sentence) => {\n                    let tokens = tokenizer.tokenizeForSentence(sentence.raw);\n                    let countableTokens = tokens.filter(token => {\n                        if (isStrict) {\n                            return is助詞Token(token);\n                        }\n                        // デフォルトでは、\"、\"を間隔値の距離としてカウントする\n                        // \"、\" があると助詞同士の距離が開くようにすることで、並列的な\"、\"の使い方を許容する目的\n                        // https://github.com/azu/textlint-rule-no-doubled-joshi/issues/2\n                        return is助詞Token(token) || is読点Token(token);\n                    });\n                    let joshiTokenSurfaceKeyMap = createSurfaceKeyMap(countableTokens);\n                    /*\n                     # Data Structure\n\n                     joshiTokens = [tokenA, tokenB, tokenC, tokenD, tokenE, tokenF]\n                     joshiTokenSurfaceKeyMap = {\n                     \"は:助詞.係助詞\": [tokenA, tokenC, tokenE],\n                     \"で:助詞.係助詞\": [tokenB, tokenD, tokenF]\n                     }\n                     */\n                    Object.keys(joshiTokenSurfaceKeyMap).forEach(key => {\n                        const tokens = joshiTokenSurfaceKeyMap[key];\n                        const joshiName = restoreToSurfaceFromKey(key);\n                        // check allow\n                        if (allow.indexOf(joshiName) >= 0) {\n                            return;\n                        }\n                        // strict mode ではない時例外を除去する\n                        if (!isStrict) {\n                            if (matchExceptionRule(tokens)) {\n                                return;\n                            }\n                        }\n                        if (tokens.length <= 1) {\n                            return;// no duplicated token\n                        }\n                        // if found differenceIndex less than\n                        // tokes are sorted ascending order\n                        tokens.reduce((prev, current) => {\n                            const startPosition = countableTokens.indexOf(prev);\n                            const otherPosition = countableTokens.indexOf(current);\n                            // 助詞token同士の距離が設定値以下ならエラーを報告する\n                            const differenceIndex = otherPosition - startPosition;\n                            if (differenceIndex <= minInterval) {\n                                const originalPosition = source.originalPositionFor({\n                                    line: sentence.loc.start.line,\n                                    column: sentence.loc.start.column + (current.word_position - 1)\n                                });\n                                // padding positionを計算する\n                                const padding = {\n                                    line: originalPosition.line - 1,\n                                    // matchLastToken.word_position start with 1\n                                    // this is padding column start with 0 (== -1)\n                                    column: originalPosition.column\n                                };\n                                report(node, new RuleError(`一文に二回以上利用されている助詞 \"${joshiName}\" がみつかりました。`, padding));\n                            }\n                            return current;\n                        });\n                    });\n                };\n                sentences.forEach(checkSentence);\n            });\n        }\n    }\n};\n"]}